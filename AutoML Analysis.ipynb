{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c144aec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tpot in c:\\users\\pptma\\anaconda3\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: tqdm>=4.36.1 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (4.62.2)\n",
      "Requirement already satisfied: numpy>=1.16.3 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.0.1)\n",
      "Requirement already satisfied: stopit>=1.1.1 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.1.2)\n",
      "Requirement already satisfied: scipy>=1.3.1 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.0 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.0.1)\n",
      "Requirement already satisfied: xgboost>=1.1.0 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.5.1)\n",
      "Requirement already satisfied: deap>=1.2 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.3.1)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (1.2.4)\n",
      "Requirement already satisfied: update-checker>=0.16 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tpot) (0.18.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from pandas>=0.24.2->tpot) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->tpot) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from scikit-learn>=0.22.0->tpot) (2.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from tqdm>=4.36.1->tpot) (0.4.4)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from update-checker>=0.16->tpot) (2.25.1)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->tpot) (2020.12.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in c:\\users\\pptma\\anaconda3\\lib\\site-packages (1.10.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pip-utils in c:\\users\\pptma\\anaconda3\\lib\\site-packages (0.0.6)\n",
      "Requirement already satisfied: pip>=8.0.0 in c:\\users\\pptma\\anaconda3\\lib\\site-packages (from pip-utils) (21.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tpot\n",
    "%pip install torch\n",
    "%pip install pip-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bec51c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tpot import TPOTClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5692d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i in range(7):\n",
    "    data = pd.read_csv('FIFA-Data/Cleaned Data/new_players_' + str(15 + i) + '.csv').set_index('sofifa_id')\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6dda0f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndata_list = []\n",
    "for i in range(6):\n",
    "    next_yr_value = data_list[i+1]['value_eur']\n",
    "    #ndata_list.append(data_list[i].join(next_yr_value, how='left', rsuffix='_next').fillna(0))\n",
    "    ndata_list.append(data_list[i])\n",
    "    ndata_list[i]['value_eur_next'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ba5f7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Outfielder Data\n",
    "ndata_list2 = []\n",
    "for i, j in enumerate(ndata_list):\n",
    "    ndata_list2.append(j.where(j['value_eur_next']!=0.0).dropna(how='all'))\n",
    "    ndata_list2[i] = ndata_list2[i].where(ndata_list2[i]['team_position']!='GK').dropna(how='all')\n",
    "\n",
    "tier1 = ['English Premier League','Spain Primera Division','Italian Serie A',\n",
    "        'German 1. Bundesliga', 'French Ligue 1', 'Russian Premier League',\n",
    "        'Portuguese Liga ZON SAGRES', 'Belgian Jupiler Pro League',\n",
    "        'Holland Eredivisie', 'USA Major League Soccer']\n",
    "\n",
    "tier2 = ['Turkish Süper Lig','Mexican Liga MX','Campeonato Brasileiro Série A',\n",
    "        'Scottish Premiership', 'Argentina Primera División',\n",
    "        'Austrian Football Bundesliga', 'Ukrainian Premier League',\n",
    "        'Danish Superliga', 'Swiss Super League', 'Rest of World']\n",
    "\n",
    "tier3 = ['Croatian Prva HNL','English League Championship', 'Greek Super League', \n",
    "        'Swedish Allsvenskan', 'Czech Republic Gambrinus Liga',\n",
    "        'Saudi Abdul L. Jameel League', 'Polish T-Mobile Ekstraklasa',\n",
    "        'Chinese Super League', 'Colombian Liga Postobón',\n",
    "        'Japanese J. League Division 1']\n",
    "\n",
    "tier4 = ['Norwegian Eliteserien','Romanian Liga I','0',\n",
    "        'German 2. Bundesliga','Spanish Segunda División', \n",
    "        'Chilian Campeonato Nacional','Australian Hyundai A-League', \n",
    "        'Italian Serie B', 'French Ligue 2',\n",
    "        'South African Premier Division', \n",
    "        'English League One', 'Korean K League Classic',\n",
    "        'German 3. Bundesliga', 'English League Two',\n",
    "        'Finnish Veikkausliiga', 'Rep. Ireland Airtricity League']\n",
    "\n",
    "rankinggk = [[],[],[],[],[],[]]\n",
    "for j in range(6):\n",
    "    for i in ndata_list2[j]['league_name']:\n",
    "        if i in tier1:\n",
    "            rankinggk[j].append(1)\n",
    "        elif i in tier2:\n",
    "            rankinggk[j].append(2)\n",
    "        elif i in tier3:\n",
    "            rankinggk[j].append(3)\n",
    "        elif i in tier4:\n",
    "            rankinggk[j].append(4)\n",
    "        else:\n",
    "            rankinggk[j].append(5)\n",
    "\n",
    "for i in range(len(ndata_list2)):\n",
    "    ndata_list2[i].insert(10,'league_ranking',rankinggk[i])\n",
    "    \n",
    "for i in range(len(ndata_list2)):\n",
    "    ndata_list2[i].insert(8,'year',np.ones(len(ndata_list2[i]))*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eaaeb7b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height_cm</th>\n",
       "      <th>weight_kg</th>\n",
       "      <th>overall</th>\n",
       "      <th>potential</th>\n",
       "      <th>pace</th>\n",
       "      <th>shooting</th>\n",
       "      <th>passing</th>\n",
       "      <th>defending</th>\n",
       "      <th>dribbling</th>\n",
       "      <th>physic</th>\n",
       "      <th>year</th>\n",
       "      <th>league_ranking</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sofifa_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179712</th>\n",
       "      <td>25.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185204</th>\n",
       "      <td>28.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202423</th>\n",
       "      <td>25.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192310</th>\n",
       "      <td>25.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190483</th>\n",
       "      <td>23.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239728</th>\n",
       "      <td>24.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232593</th>\n",
       "      <td>25.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229294</th>\n",
       "      <td>28.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222864</th>\n",
       "      <td>24.0</td>\n",
       "      <td>191.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252527</th>\n",
       "      <td>18.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45733 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age  height_cm  weight_kg  overall  potential  pace  shooting  \\\n",
       "sofifa_id                                                                   \n",
       "179712     25.0      175.0       67.0     71.0       75.0  78.0      67.0   \n",
       "185204     28.0      192.0       84.0     69.0       69.0  53.0      59.0   \n",
       "202423     25.0      180.0       79.0     56.0       62.0  60.0      45.0   \n",
       "192310     25.0      184.0       77.0     68.0       71.0  78.0      58.0   \n",
       "190483     23.0      170.0       73.0     80.0       84.0  85.0      73.0   \n",
       "...         ...        ...        ...      ...        ...   ...       ...   \n",
       "239728     24.0      183.0       73.0     64.0       66.0  68.0      62.0   \n",
       "232593     25.0      185.0       74.0     69.0       72.0  77.0      68.0   \n",
       "229294     28.0      193.0       89.0     70.0       70.0  50.0      30.0   \n",
       "222864     24.0      191.0       75.0     62.0       68.0   0.0       0.0   \n",
       "252527     18.0      182.0       74.0     53.0       73.0  61.0      23.0   \n",
       "\n",
       "           passing  defending  dribbling  physic  year  league_ranking  \n",
       "sofifa_id                                                               \n",
       "179712        68.0       32.0       76.0    55.0   0.0               1  \n",
       "185204        63.0       71.0       51.0    75.0   0.0               2  \n",
       "202423        51.0       29.0       58.0    58.0   0.0               1  \n",
       "192310        61.0       66.0       66.0    73.0   0.0               1  \n",
       "190483        78.0       51.0       85.0    62.0   0.0               2  \n",
       "...            ...        ...        ...     ...   ...             ...  \n",
       "239728        59.0       40.0       64.0    69.0   5.0               3  \n",
       "232593        51.0       18.0       65.0    58.0   5.0               3  \n",
       "229294        45.0       70.0       41.0    79.0   5.0               4  \n",
       "222864         0.0        0.0        0.0     0.0   5.0               4  \n",
       "252527        28.0       51.0       35.0    63.0   5.0               2  \n",
       "\n",
       "[45733 rows x 13 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Below is the set of columns we care about\n",
    "\n",
    "features = ['age'\n",
    "            ,'height_cm'\n",
    "           ,'weight_kg'\n",
    "           ,'overall'\n",
    "           ,'potential'\n",
    "           ,'pace'\n",
    "           ,'shooting'\n",
    "           ,'passing'\n",
    "           ,'defending'\n",
    "           ,'dribbling'\n",
    "           ,'physic'\n",
    "           ,'year'\n",
    "           ,'league_ranking']\n",
    "\n",
    "filtered_data = []\n",
    "ys = []\n",
    "\n",
    "for d in ndata_list2:\n",
    "    filtered_data.append(d[features].fillna(0))\n",
    "    ys.append(d['value_eur_next'].fillna(0))\n",
    "\n",
    "# Merging data across years\n",
    "for i in range(len(filtered_data)):\n",
    "    n = len(filtered_data[i])\n",
    "    t1 = int(0.65*n)\n",
    "    t2 = int(0.8*n)\n",
    "    index_shuffle = np.array(filtered_data[i].index)\n",
    "    np.random.shuffle(index_shuffle)\n",
    "\n",
    "    train_x_i = filtered_data[i].loc[index_shuffle[0:t1]]\n",
    "    validate_x_i = filtered_data[i].loc[index_shuffle[t1:t2]]\n",
    "    test_x_i = filtered_data[i].loc[index_shuffle[t2:n]]\n",
    "\n",
    "    train_y_i = ys[i].loc[index_shuffle[0:t1]]\n",
    "    validate_y_i = ys[i].loc[index_shuffle[t1:t2]]\n",
    "    test_y_i = ys[i].loc[index_shuffle[t2:n]]\n",
    "    \n",
    "    train_log_y_i = np.log(train_y_i)\n",
    "    validate_log_y_i = np.log(validate_y_i)\n",
    "    test_log_y_i = np.log(test_y_i)\n",
    "    train_log_y_i.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "    validate_log_y_i.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "    test_log_y_i.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "    if i == 0:\n",
    "        train_x = train_x_i\n",
    "        validate_x = validate_x_i\n",
    "        test_x = test_x_i\n",
    "        train_y = train_y_i\n",
    "        validate_y = validate_y_i\n",
    "        test_y = test_y_i        \n",
    "        train_log_y = train_log_y_i\n",
    "        validate_log_y = validate_log_y_i\n",
    "        test_log_y = test_log_y_i  \n",
    "    else:\n",
    "        train_x = train_x.append(train_x_i)\n",
    "        validate_x = validate_x.append(validate_x_i)\n",
    "        test_x = test_x.append(test_x_i)\n",
    "        train_y = train_y.append(train_y_i)\n",
    "        validate_y = validate_y.append(validate_y_i)\n",
    "        test_y = test_y.append(test_y_i)        \n",
    "        train_log_y = train_log_y.append(train_log_y_i)\n",
    "        validate_log_y = validate_log_y.append(validate_log_y_i)\n",
    "        test_log_y = test_log_y.append(test_log_y_i)\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_x,train_y)\n",
    "    pred = model.coef_.reshape(13,1)\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "778f82b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "416bfbb1803a450ba81d97d788f7f059",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.68 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(input_matrix, bootstrap=False, max_features=0.5, min_samples_leaf=7, min_samples_split=2, n_estimators=100)\n",
      "Test Set Error:  982764.793111172\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TPOT Regressor - Outfielders\n",
    "\n",
    "tpot1 = TPOTRegressor(generations=5, population_size=20, verbosity=2, max_time_mins=1.5)\n",
    "tpot1.fit(train_x, train_y)\n",
    "pred = tpot1.predict(test_x)\n",
    "print(\"Test Set Error: \", str(np.mean(np.abs(test_y - np.array(pred).reshape(len(pred),)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e5e86cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Error:  1131130.159857904\n"
     ]
    }
   ],
   "source": [
    "# TPOT Classifier - Outfielders\n",
    "\n",
    "tpot2 = TPOTClassifier(generations=5, population_size=20, verbosity=2, max_time_mins=1.5)\n",
    "tpot2.fit(train_x, train_y)\n",
    "pred = tpot2.predict(test_x)\n",
    "print(\"Test Set Error: \", str(np.mean(np.abs(test_y - np.array(pred).reshape(len(pred),)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "acc865b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635d81d176364c3fbc015835611cc496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7600291019149059\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7634025958908096\n",
      "\n",
      "1.50 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=gini, max_features=0.4, min_samples_leaf=4, min_samples_split=14, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bce24cd223654a7eab7d6740f28dea3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.7035096909376637\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.7035096909376637\n",
      "\n",
      "1.54 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: GaussianNB(XGBClassifier(input_matrix, learning_rate=0.1, max_depth=10, min_child_weight=12, n_estimators=100, n_jobs=1, subsample=0.45, verbosity=0))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d67710503d4431bc80e1c40c8a457b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.6494511378848727\n",
      "\n",
      "1.53 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.5, min_samples_leaf=2, min_samples_split=20, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f302c23259048f08618af80e175993f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.574122577265584\n",
      "\n",
      "1.54 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(ZeroCount(input_matrix), bootstrap=False, criterion=entropy, max_features=0.8500000000000001, min_samples_leaf=18, min_samples_split=8, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c76cde5381fb4de483356c9e32ec0d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.5514184273325184\n",
      "\n",
      "1.53 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.7000000000000001, min_samples_leaf=16, min_samples_split=2, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eeefd5413664ca8adad46f7529291c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.51 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.9500000000000001, min_samples_leaf=9, min_samples_split=3, n_estimators=100)\n",
      "Test Set Error:  0 \t 1.185907436297452\n",
      "Test Set Error:  1 \t 0.9550182007280291\n",
      "Test Set Error:  2 \t 0.8619344773790951\n",
      "Test Set Error:  3 \t 0.9175767030681228\n",
      "Test Set Error:  4 \t 1.015340613624545\n",
      "Test Set Error:  5 \t 1.388715548621945\n"
     ]
    }
   ],
   "source": [
    "# TPOT Classifier - Outfielders with bins\n",
    "\n",
    "bins_try= [[0,600000,2500000,150000000],[0,500000,1250000,5000000,150000000],[0,400000,900000,2500000,10000000,150000000],\n",
    "           [0,300000,700000,1250000,3000000,8000000,150000000],[0,250000,600000,1000000,2000000,4000000,10000000,150000000],\n",
    "          [0,200000,500000,800000,1250000,2250000,4500000,10000000,150000000]]\n",
    "\n",
    "import copy\n",
    "ys_bins1 = copy.deepcopy(ys) #3\n",
    "ys_bins2 = copy.deepcopy(ys) #4\n",
    "ys_bins3 = copy.deepcopy(ys) #5\n",
    "ys_bins4 = copy.deepcopy(ys) #6\n",
    "ys_bins5 = copy.deepcopy(ys) #7\n",
    "ys_bins6 = copy.deepcopy(ys) #8\n",
    "ys_bins = [ys_bins1,ys_bins2,ys_bins3,ys_bins4,ys_bins5,ys_bins6]\n",
    "for h in range(len(ys_bins)):\n",
    "    for i in range(len(ys_bins[h])):\n",
    "        for j in ys_bins[h][i].index:\n",
    "            for k, l in enumerate(bins_try[h]):\n",
    "                if l >= ys_bins[h][i][j]:\n",
    "                    ys_bins[h][i][j] = k\n",
    "                    break\n",
    "                    \n",
    "train_y = [[],[],[],[],[],[]]\n",
    "validate_y = [[],[],[],[],[],[]]\n",
    "test_y = [[],[],[],[],[],[]]\n",
    "for i in range(len(filtered_data)):\n",
    "    n = len(filtered_data[i])\n",
    "    t1 = int(0.65*n)\n",
    "    t2 = int(0.8*n)\n",
    "    index_shuffle = np.array(filtered_data[i].index)\n",
    "    np.random.shuffle(index_shuffle)\n",
    "\n",
    "    train_x_i = filtered_data[i].loc[index_shuffle[0:t1]]\n",
    "    validate_x_i = filtered_data[i].loc[index_shuffle[t1:t2]]\n",
    "    test_x_i = filtered_data[i].loc[index_shuffle[t2:n]]\n",
    "    if i == 0:\n",
    "        train_x = train_x_i\n",
    "        validate_x = validate_x_i\n",
    "        test_x = test_x_i\n",
    "    else:\n",
    "        train_x = train_x.append(train_x_i)\n",
    "        validate_x = validate_x.append(validate_x_i)\n",
    "        test_x = test_x.append(test_x_i)\n",
    "    for j in range(len(ys_bins)):\n",
    "        train_y_i = ys_bins[j][i].loc[index_shuffle[0:t1]]\n",
    "        validate_y_i = ys_bins[j][i].loc[index_shuffle[t1:t2]]\n",
    "        test_y_i = ys_bins[j][i].loc[index_shuffle[t2:n]]\n",
    "        if i == 0:\n",
    "            train_y[j] = train_y_i\n",
    "            validate_y[j] = validate_y_i\n",
    "            test_y[j] = test_y_i\n",
    "        else:\n",
    "            train_y[j] = train_y[j].append(train_y_i)\n",
    "            validate_y[j] = validate_y[j].append(validate_y_i)\n",
    "            test_y[j] = test_y[j].append(test_y_i)\n",
    "\n",
    "preds_c = []\n",
    "for h in range(len(ys_bins)):\n",
    "    tpot_c1 = TPOTClassifier(generations=5, population_size=20, verbosity=2, max_time_mins=1.5)\n",
    "    tpot_c1.fit(train_x, train_y[h])\n",
    "    preds_c.append((h, tpot_c1.predict(test_x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ed22d8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Error:  0 \t 0.8065522620904836\n",
      "Test Set Error:  1 \t 0.750390015600624\n",
      "Test Set Error:  2 \t 0.6786271450858035\n",
      "Test Set Error:  3 \t 0.5741029641185648\n",
      "Test Set Error:  4 \t 0.5647425897035881\n",
      "Test Set Error:  5 \t 0.5195007800312013\n"
     ]
    }
   ],
   "source": [
    "for h in range(len(preds_c)):\n",
    "    print(\"Test Set Error: \", str(preds_c[h][0]),\"\\t\", str(np.sum(preds_c[h][1]==test_y[h])/len(test_y[h])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7694b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Goalkeeper only data\n",
    "ndata_list3 = []\n",
    "for i, j in enumerate(ndata_list):\n",
    "    ndata_list3.append(j.where(j['value_eur_next']!=0.0).dropna(how='all'))\n",
    "    ndata_list3[i] = ndata_list3[i].where(ndata_list3[i]['team_position']=='GK').dropna(how='all')\n",
    "\n",
    "rankinggk = [[],[],[],[],[],[]]\n",
    "for j in range(6):\n",
    "    for i in ndata_list3[j]['league_name']:\n",
    "        if i in tier1:\n",
    "            rankinggk[j].append(1)\n",
    "        elif i in tier2:\n",
    "            rankinggk[j].append(2)\n",
    "        elif i in tier3:\n",
    "            rankinggk[j].append(3)\n",
    "        elif i in tier4:\n",
    "            rankinggk[j].append(4)\n",
    "        else:\n",
    "            rankinggk[j].append(5)\n",
    "\n",
    "for i in range(len(ndata_list3)):\n",
    "    ndata_list3[i].insert(10,'league_ranking',rankinggk[i])\n",
    "    \n",
    "for i in range(len(ndata_list3)):\n",
    "    ndata_list3[i].insert(8,'year',np.ones(len(ndata_list3[i]))*i)\n",
    "\n",
    "features = ['age'\n",
    "            ,'height_cm'\n",
    "           ,'weight_kg'\n",
    "           ,'overall'\n",
    "           ,'potential'\n",
    "           ,'gk_diving'\n",
    "           ,'gk_reflexes'\n",
    "           ,'gk_handling'\n",
    "           ,'gk_kicking'\n",
    "           ,'gk_speed'\n",
    "           ,'gk_positioning'\n",
    "           ,'year'\n",
    "           ,'league_ranking']\n",
    "\n",
    "filtered_data = []\n",
    "ys = []\n",
    "for d in ndata_list3:\n",
    "    filtered_data.append(d[features].fillna(0))\n",
    "    ys.append(d['value_eur_next'].fillna(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a52c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPOT Regressor - Goalkeepers\n",
    "\n",
    "tpot3 = TPOTRegressor(generations=5, population_size=20, verbosity=2, max_time_mins=1.5)\n",
    "tpot3.fit(train_x, train_y)\n",
    "pred = tpot3.predict(test_x)\n",
    "print(\"Test Set Error: \", str(np.mean(np.abs(test_y - np.array(pred).reshape(len(pred),)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a300faa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44bee7d97b8d4e8cb408557f74613e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.83 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.8500000000000001, min_samples_leaf=7, min_samples_split=8, n_estimators=100)\n",
      "Test Set Error:  1020633.5346358792\n"
     ]
    }
   ],
   "source": [
    "# TPOT Classifier - Goalkeepers\n",
    "\n",
    "tpot3 = TPOTClassifier(generations=5, population_size=20, verbosity=2, max_time_mins=1.5)\n",
    "tpot3.fit(train_x, train_y)\n",
    "pred = tpot3.predict(test_x)\n",
    "print(\"Test Set Error: \", str(np.mean(np.abs(test_y - np.array(pred).reshape(len(pred),)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6795ec89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b0491d42348434e99cf105d86c3e51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.54 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(PolynomialFeatures(input_matrix, degree=2, include_bias=False, interaction_only=False), learning_rate=0.01, max_depth=7, max_features=0.05, min_samples_leaf=18, min_samples_split=2, n_estimators=100, subsample=1.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1852ad7661c4f8a8d95a3c9f182845d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.6962877597345905\n",
      "\n",
      "1.51 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.55, min_samples_leaf=6, min_samples_split=5, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd7f6ddbacc4ee68819bbbb5ec4b577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.71 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: GradientBoostingClassifier(input_matrix, learning_rate=0.1, max_depth=1, max_features=0.8500000000000001, min_samples_leaf=11, min_samples_split=7, n_estimators=100, subsample=0.4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30faa95e57a04ce98c475efad0a2556e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.53 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=False, criterion=entropy, max_features=0.55, min_samples_leaf=1, min_samples_split=16, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e132e9007f04c52bedb9e8dafa1ff5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.54 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.55, min_samples_leaf=15, min_samples_split=6, n_estimators=100)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54ad6fc8238c4f60b8206cb6995b58f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/20 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.62 minutes have elapsed. TPOT will close down.\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n",
      "\n",
      "Best pipeline: ExtraTreesClassifier(input_matrix, bootstrap=True, criterion=gini, max_features=0.7000000000000001, min_samples_leaf=2, min_samples_split=18, n_estimators=100)\n",
      "Test Set Error:  0 \t 0.7800312012480499\n",
      "Test Set Error:  1 \t 0.7145085803432137\n",
      "Test Set Error:  2 \t 0.6458658346333853\n",
      "Test Set Error:  3 \t 0.5881435257410297\n",
      "Test Set Error:  4 \t 0.5756630265210608\n",
      "Test Set Error:  5 \t 0.516380655226209\n"
     ]
    }
   ],
   "source": [
    "# TPOT Classifier - Goalkeepers with bins\n",
    "\n",
    "bins_try= [[0,600000,2500000,150000000],[0,500000,1250000,5000000,150000000],[0,400000,900000,2500000,10000000,150000000],\n",
    "           [0,300000,700000,1250000,3000000,8000000,150000000],[0,250000,600000,1000000,2000000,4000000,10000000,150000000],\n",
    "          [0,200000,500000,800000,1250000,2250000,4500000,10000000,150000000]]\n",
    "\n",
    "import copy\n",
    "ys_bins1 = copy.deepcopy(ys) #3\n",
    "ys_bins2 = copy.deepcopy(ys) #4\n",
    "ys_bins3 = copy.deepcopy(ys) #5\n",
    "ys_bins4 = copy.deepcopy(ys) #6\n",
    "ys_bins5 = copy.deepcopy(ys) #7\n",
    "ys_bins6 = copy.deepcopy(ys) #8\n",
    "ys_bins = [ys_bins1,ys_bins2,ys_bins3,ys_bins4,ys_bins5,ys_bins6]\n",
    "for h in range(len(ys_bins)):\n",
    "    for i in range(len(ys_bins[h])):\n",
    "        for j in ys_bins[h][i].index:\n",
    "            for k, l in enumerate(bins_try[h]):\n",
    "                if l >= ys_bins[h][i][j]:\n",
    "                    ys_bins[h][i][j] = k\n",
    "                    break\n",
    "                    \n",
    "train_y = [[],[],[],[],[],[]]\n",
    "validate_y = [[],[],[],[],[],[]]\n",
    "test_y = [[],[],[],[],[],[]]\n",
    "for i in range(len(filtered_data)):\n",
    "    n = len(filtered_data[i])\n",
    "    t1 = int(0.65*n)\n",
    "    t2 = int(0.8*n)\n",
    "    index_shuffle = np.array(filtered_data[i].index)\n",
    "    np.random.shuffle(index_shuffle)\n",
    "\n",
    "    train_x_i = filtered_data[i].loc[index_shuffle[0:t1]]\n",
    "    validate_x_i = filtered_data[i].loc[index_shuffle[t1:t2]]\n",
    "    test_x_i = filtered_data[i].loc[index_shuffle[t2:n]]\n",
    "    if i == 0:\n",
    "        train_x = train_x_i\n",
    "        validate_x = validate_x_i\n",
    "        test_x = test_x_i\n",
    "    else:\n",
    "        train_x = train_x.append(train_x_i)\n",
    "        validate_x = validate_x.append(validate_x_i)\n",
    "        test_x = test_x.append(test_x_i)\n",
    "    for j in range(len(ys_bins)):\n",
    "        train_y_i = ys_bins[j][i].loc[index_shuffle[0:t1]]\n",
    "        validate_y_i = ys_bins[j][i].loc[index_shuffle[t1:t2]]\n",
    "        test_y_i = ys_bins[j][i].loc[index_shuffle[t2:n]]\n",
    "        if i == 0:\n",
    "            train_y[j] = train_y_i\n",
    "            validate_y[j] = validate_y_i\n",
    "            test_y[j] = test_y_i\n",
    "        else:\n",
    "            train_y[j] = train_y[j].append(train_y_i)\n",
    "            validate_y[j] = validate_y[j].append(validate_y_i)\n",
    "            test_y[j] = test_y[j].append(test_y_i)\n",
    "\n",
    "preds_c = []\n",
    "for h in range(len(ys_bins)):\n",
    "    tpot_c2 = TPOTClassifier(generations=5, population_size=20, verbosity=2, max_time_mins=1.5)\n",
    "    tpot_c2.fit(train_x, train_y[h])\n",
    "    preds_c.append((h, tpot_c2.predict(test_x)))\n",
    "\n",
    "for h in range(len(preds_c)):\n",
    "    print(\"Test Set Error: \", str(preds_c[h][0]),\"\\t\", str(np.sum(preds_c[h][1]==test_y[h])/len(test_y[h])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
